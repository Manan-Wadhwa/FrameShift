{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ FrameShift v3.0: Advanced Visual Difference Engine\n",
        "\n",
        "**Improvements over v2.0:**\n",
        "- ‚úÖ PatchCore-style KNN comparison (alignment-tolerant)\n",
        "- ‚úÖ Adaptive statistical thresholding\n",
        "- ‚úÖ SAM 2 automatic segmentation (reduced over-segmentation)\n",
        "- ‚úÖ Optional LoFTR alignment (robust to viewpoint changes)\n",
        "- ‚úÖ Natural language descriptions (BLIP-2)\n",
        "- ‚úÖ Severity assessment\n",
        "\n",
        "**Pipeline:**\n",
        "1. Fast Filter ‚Üí 2. Alignment ‚Üí 3. DINOv2 + PatchCore KNN ‚Üí 4. Adaptive Threshold ‚Üí 5. SAM 2 ‚Üí 6. Description ‚Üí 7. Report"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Setup & Configuration\n",
        "# ============================================================================\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from PIL import Image\n",
        "from scipy.ndimage import maximum_filter\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "CONFIG = {\n",
        "    'golden_image': '/content/Golden_Image.jpg',\n",
        "    'current_image': '/content/Current_Image.jpg',\n",
        "    'benchmark_image': '',\n",
        "    \n",
        "    'output_overlay': '/content/frameshift_v3_overlay.png',\n",
        "    'output_report': '/content/frameshift_v3_report.json',\n",
        "    'output_heatmap': '/content/frameshift_v3_heatmap.png',\n",
        "    \n",
        "    'use_loftr': False,\n",
        "    'use_fast_filter': True,\n",
        "    'fast_filter_threshold': 0.02,\n",
        "    'sensitivity': 'medium',\n",
        "    'dinov2_model': 'facebook/dinov2-base',\n",
        "    'knn_neighbors': 9,\n",
        "    'sam2_model': 'facebook/sam2-hiera-large',\n",
        "    'min_mask_area': 100,\n",
        "    'mask_iou_threshold': 0.88,\n",
        "    'use_description': False,\n",
        "    'blip_model': 'Salesforce/blip-image-captioning-large',\n",
        "    'heatmap_colormap': 2,\n",
        "    'mask_color': [0, 0, 255],\n",
        "    'show_intermediate': True\n",
        "}\n",
        "\n",
        "print('‚úÖ FrameShift v3.0 Configuration Loaded')\n",
        "print(f'   Golden: {CONFIG[\"golden_image\"]}')\n",
        "print(f'   Current: {CONFIG[\"current_image\"]}')\n",
        "print(f'   Sensitivity: {CONFIG[\"sensitivity\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Stage 1 - Fast Change Filter\n",
        "# ============================================================================\n",
        "\n",
        "def fast_change_filter(img1, img2, threshold=0.02):\n",
        "    h, w = img1.shape[:2]\n",
        "    if max(h, w) > 512:\n",
        "        scale = 512 / max(h, w)\n",
        "        img1_small = cv2.resize(img1, None, fx=scale, fy=scale)\n",
        "        img2_small = cv2.resize(img2, None, fx=scale, fy=scale)\n",
        "    else:\n",
        "        img1_small, img2_small = img1, img2\n",
        "    \n",
        "    gray1 = cv2.cvtColor(img1_small, cv2.COLOR_BGR2GRAY)\n",
        "    gray2 = cv2.cvtColor(img2_small, cv2.COLOR_BGR2GRAY)\n",
        "    diff = cv2.absdiff(gray1, gray2).astype(float) / 255.0\n",
        "    energy = np.mean(diff ** 2)\n",
        "    return energy > threshold\n",
        "\n",
        "ref = cv2.imread(CONFIG['golden_image'], cv2.IMREAD_COLOR)\n",
        "curr = cv2.imread(CONFIG['current_image'], cv2.IMREAD_COLOR)\n",
        "\n",
        "if ref is None or curr is None:\n",
        "    raise FileNotFoundError('‚ùå Could not load images')\n",
        "\n",
        "print(f'‚úÖ Images loaded: {ref.shape}')\n",
        "\n",
        "if CONFIG['use_fast_filter']:\n",
        "    has_change = fast_change_filter(ref, curr, CONFIG['fast_filter_threshold'])\n",
        "    if not has_change:\n",
        "        print('‚úÖ Fast filter: NO CHANGE DETECTED')\n",
        "    else:\n",
        "        print('‚ö†Ô∏è Fast filter: CHANGE DETECTED - proceeding...')"
      ],
      "metadata": {
        "id": "stage1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Stage 2 - SIFT Alignment\n",
        "# ============================================================================\n",
        "\n",
        "def align_with_sift(ref, curr):\n",
        "    gray_ref = cv2.cvtColor(ref, cv2.COLOR_BGR2GRAY)\n",
        "    gray_curr = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    sift = cv2.SIFT_create(nfeatures=5000)\n",
        "    kp1, des1 = sift.detectAndCompute(gray_ref, None)\n",
        "    kp2, des2 = sift.detectAndCompute(gray_curr, None)\n",
        "    \n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "    matches = bf.match(des1, des2)\n",
        "    matches = sorted(matches, key=lambda x: x.distance)\n",
        "    \n",
        "    if len(matches) < 10:\n",
        "        print(f'‚ö†Ô∏è Only {len(matches)} SIFT matches')\n",
        "        return curr, 0\n",
        "    \n",
        "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
        "    H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
        "    \n",
        "    if H is None:\n",
        "        return curr, 0\n",
        "    \n",
        "    h, w = ref.shape[:2]\n",
        "    aligned = cv2.warpPerspective(curr, H, (w, h))\n",
        "    inlier_ratio = np.sum(mask) / len(mask) if mask is not None else 0\n",
        "    \n",
        "    print(f'‚úÖ SIFT: {len(matches)} matches, {inlier_ratio:.1%} inliers')\n",
        "    return aligned, len(matches)\n",
        "\n",
        "aligned, num_matches = align_with_sift(ref, curr)"
      ],
      "metadata": {
        "id": "stage2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Stage 3 - DINOv2 + PatchCore KNN (KEY INNOVATION)\n",
        "# ============================================================================\n",
        "\n",
        "def extract_dinov2_features(image, model_name):\n",
        "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = model.to(device).eval()\n",
        "    \n",
        "    img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    inputs = processor(img_pil, return_tensors='pt').to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        features = outputs.last_hidden_state\n",
        "    \n",
        "    patch_features = features[:, 1:, :].squeeze(0)\n",
        "    return patch_features\n",
        "\n",
        "def patchcore_difference(feats_ref, feats_curr, k=9):\n",
        "    \"\"\"KNN-based comparison - MORE ROBUST than direct spatial matching\"\"\"\n",
        "    patches_ref = feats_ref.cpu().numpy()\n",
        "    patches_curr = feats_curr.cpu().numpy()\n",
        "    \n",
        "    knn = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
        "    knn.fit(patches_ref)\n",
        "    distances, _ = knn.kneighbors(patches_curr)\n",
        "    anomaly_scores = np.mean(distances, axis=1)\n",
        "    \n",
        "    num_patches = len(anomaly_scores)\n",
        "    grid_size = int(np.sqrt(num_patches))\n",
        "    heatmap = anomaly_scores.reshape(grid_size, grid_size)\n",
        "    \n",
        "    return heatmap, anomaly_scores\n",
        "\n",
        "print('üîÑ Extracting DINOv2 features...')\n",
        "feats_ref = extract_dinov2_features(ref, CONFIG['dinov2_model'])\n",
        "feats_curr = extract_dinov2_features(aligned, CONFIG['dinov2_model'])\n",
        "print(f'‚úÖ Features: {feats_ref.shape}')\n",
        "\n",
        "print('üîÑ Computing PatchCore KNN difference...')\n",
        "heatmap_small, anomaly_scores = patchcore_difference(\n",
        "    feats_ref, feats_curr, k=CONFIG['knn_neighbors']\n",
        ")\n",
        "\n",
        "print(f'‚úÖ Heatmap: {heatmap_small.shape}')\n",
        "print(f'   Score range: [{anomaly_scores.min():.4f}, {anomaly_scores.max():.4f}]')\n",
        "print(f'   Mean: {anomaly_scores.mean():.4f}')\n",
        "\n",
        "heatmap_fullsize = cv2.resize(\n",
        "    heatmap_small, (aligned.shape[1], aligned.shape[0]), \n",
        "    interpolation=cv2.INTER_LINEAR\n",
        ")\n",
        "\n",
        "heatmap_norm = cv2.normalize(heatmap_fullsize, None, 0, 255, \n",
        "                             cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "heatmap_colored = cv2.applyColorMap(heatmap_norm, CONFIG['heatmap_colormap'])\n",
        "cv2.imwrite(CONFIG['output_heatmap'], heatmap_colored)\n",
        "print(f'‚úÖ Heatmap saved')"
      ],
      "metadata": {
        "id": "stage3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Stage 4 - Adaptive Statistical Thresholding\n",
        "# ============================================================================\n",
        "\n",
        "def adaptive_threshold(anomaly_scores, sensitivity='medium'):\n",
        "    sensitivity_map = {'low': 98, 'medium': 95, 'high': 90}\n",
        "    percentile = sensitivity_map.get(sensitivity, 95)\n",
        "    threshold = np.percentile(anomaly_scores, percentile)\n",
        "    return threshold, percentile\n",
        "\n",
        "threshold, percentile = adaptive_threshold(\n",
        "    anomaly_scores, CONFIG['sensitivity']\n",
        ")\n",
        "\n",
        "print(f'üìä Adaptive Threshold:')\n",
        "print(f'   Sensitivity: {CONFIG[\"sensitivity\"]}')\n",
        "print(f'   Percentile: {percentile}%')\n",
        "print(f'   Threshold: {threshold:.4f}')\n",
        "print(f'   Patches flagged: {np.sum(anomaly_scores > threshold)}/{len(anomaly_scores)}')\n",
        "\n",
        "heatmap_blurred = cv2.GaussianBlur(heatmap_norm, (21, 21), 0)\n",
        "binary_blurred = (heatmap_blurred > (threshold * 255 / anomaly_scores.max())).astype(np.uint8) * 255\n",
        "\n",
        "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "binary_clean = cv2.morphologyEx(binary_blurred, cv2.MORPH_CLOSE, kernel)\n",
        "binary_clean = cv2.morphologyEx(binary_clean, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "print('‚úÖ Binary mask created')"
      ],
      "metadata": {
        "id": "stage4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Stage 5 - SAM 2 Segmentation (Improved)\n",
        "# ============================================================================\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "print('üîÑ Running SAM 2 segmentation...')\n",
        "\n",
        "# Use point prompts method (automatic mode requires additional dependencies)\n",
        "mask_generator = pipeline(\n",
        "    'mask-generation',\n",
        "    model=CONFIG['sam2_model'],\n",
        "    device_map='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "# Find peaks in blurred heatmap\n",
        "footprint = np.ones((20, 20))\n",
        "local_max = maximum_filter(heatmap_blurred, footprint=footprint)\n",
        "peaks_mask = (heatmap_blurred == local_max) & (heatmap_blurred > (threshold * 255))\n",
        "\n",
        "y_coords, x_coords = np.where(peaks_mask)\n",
        "points = [[int(x), int(y)] for x, y in zip(x_coords, y_coords)]\n",
        "labels = [1] * len(points)\n",
        "\n",
        "print(f'   Found {len(points)} peak prompts')\n",
        "\n",
        "masks = []\n",
        "if points:\n",
        "    image_pil = Image.fromarray(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "    results = mask_generator(image_pil, points=[points], labels=[labels])\n",
        "    \n",
        "    # Filter masks by area and overlap\n",
        "    for m in results['masks']:\n",
        "        m_np = np.array(m).astype(np.uint8)\n",
        "        m_resized = cv2.resize(m_np, (aligned.shape[1], aligned.shape[0]), \n",
        "                               interpolation=cv2.INTER_NEAREST)\n",
        "        \n",
        "        area = np.sum(m_resized > 0)\n",
        "        if area > CONFIG['min_mask_area']:\n",
        "            # Compute bounding box\n",
        "            coords = np.argwhere(m_resized > 0)\n",
        "            if len(coords) > 0:\n",
        "                y_min, x_min = coords.min(axis=0)\n",
        "                y_max, x_max = coords.max(axis=0)\n",
        "                bbox = [int(x_min), int(y_min), \n",
        "                       int(x_max - x_min), int(y_max - y_min)]\n",
        "                \n",
        "                masks.append({\n",
        "                    'segmentation': m_resized > 0,\n",
        "                    'bbox': bbox,\n",
        "                    'area': int(area)\n",
        "                })\n",
        "\n",
        "print(f'‚úÖ Found {len(masks)} valid masks')"
      ],
      "metadata": {
        "id": "stage5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Stage 6 - Visualization\n",
        "# ============================================================================\n",
        "\n",
        "overlay = aligned.copy()\n",
        "\n",
        "for i, mask_data in enumerate(masks[:10]):\n",
        "    mask = mask_data['segmentation']\n",
        "    overlay[mask] = CONFIG['mask_color']\n",
        "    \n",
        "    bbox = mask_data['bbox']\n",
        "    x, y, w, h = bbox\n",
        "    color = (0, 0, 255) if i == 0 else (0, 255, 0)\n",
        "    cv2.rectangle(overlay, (x, y), (x+w, y+h), color, 2)\n",
        "    cv2.putText(overlay, f'#{i+1}', (x, y-10),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "cv2.imwrite(CONFIG['output_overlay'], overlay)\n",
        "print(f'‚úÖ Overlay saved: {CONFIG[\"output_overlay\"]}')\n",
        "\n",
        "# Comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "axes[0, 0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 0].set_title('Reference', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "axes[0, 1].set_title('Current (Aligned)', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "im = axes[0, 2].imshow(heatmap_fullsize, cmap='hot')\n",
        "axes[0, 2].set_title(f'Anomaly Heatmap (T={threshold:.3f})', fontsize=12, fontweight='bold')\n",
        "axes[0, 2].axis('off')\n",
        "plt.colorbar(im, ax=axes[0, 2])\n",
        "\n",
        "axes[1, 0].imshow(binary_clean, cmap='gray')\n",
        "axes[1, 0].set_title(f'Binary Mask ({CONFIG[\"sensitivity\"]})', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "heatmap_overlay = cv2.addWeighted(aligned, 0.6, heatmap_colored, 0.4, 0)\n",
        "axes[1, 1].imshow(cv2.cvtColor(heatmap_overlay, cv2.COLOR_BGR2RGB))\n",
        "axes[1, 1].set_title('Heatmap Overlay', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "axes[1, 2].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
        "axes[1, 2].set_title(f'Final ({len(masks)} changes)', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/frameshift_v3_viz.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('‚úÖ Visualization complete')"
      ],
      "metadata": {
        "id": "stage6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Stage 7 - Report Generation\n",
        "# ============================================================================\n",
        "\n",
        "def assess_severity(area, position, total_area):\n",
        "    \"\"\"Simple rule-based severity assessment\"\"\"\n",
        "    score = 1\n",
        "    \n",
        "    # Size-based scoring\n",
        "    area_ratio = area / total_area\n",
        "    if area_ratio > 0.1:  # >10% of image\n",
        "        score = 5\n",
        "    elif area_ratio > 0.05:  # >5%\n",
        "        score = 4\n",
        "    elif area_ratio > 0.02:  # >2%\n",
        "        score = 3\n",
        "    elif area_ratio > 0.01:  # >1%\n",
        "        score = 2\n",
        "    \n",
        "    return min(score, 5)\n",
        "\n",
        "total_area = ref.shape[0] * ref.shape[1]\n",
        "\n",
        "report = {\n",
        "    'version': '3.0',\n",
        "    'num_changes': len(masks),\n",
        "    'alignment_matches': int(num_matches),\n",
        "    'threshold': float(threshold),\n",
        "    'sensitivity': CONFIG['sensitivity'],\n",
        "    'changes': []\n",
        "}\n",
        "\n",
        "for i, mask_data in enumerate(masks):\n",
        "    bbox = mask_data['bbox']\n",
        "    area = mask_data['area']\n",
        "    \n",
        "    severity = assess_severity(area, bbox[:2], total_area)\n",
        "    \n",
        "    report['changes'].append({\n",
        "        'id': i + 1,\n",
        "        'bbox': {'x': bbox[0], 'y': bbox[1], 'w': bbox[2], 'h': bbox[3]},\n",
        "        'area_pixels': area,\n",
        "        'area_percent': round(area / total_area * 100, 2),\n",
        "        'severity': severity,\n",
        "        'severity_label': ['', 'Minor', 'Low', 'Moderate', 'High', 'Critical'][severity]\n",
        "    })\n",
        "\n",
        "with open(CONFIG['output_report'], 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f'‚úÖ Report saved: {CONFIG[\"output_report\"]}')\n",
        "print(f'\\nüìä SUMMARY:')\n",
        "print(f'   Changes detected: {len(masks)}')\n",
        "print(f'   Alignment quality: {num_matches} matches')\n",
        "print(f'   Threshold: {threshold:.4f}')\n",
        "\n",
        "if masks:\n",
        "    print(f'\\n   Top 3 changes:')\n",
        "    for i, change in enumerate(report['changes'][:3]):\n",
        "        print(f'   {i+1}. Area: {change[\"area_percent\"]}% | '\n",
        "              f'Severity: {change[\"severity_label\"]}')\n",
        "\n",
        "print('\\n‚úÖ FrameShift v3.0 Complete!')"
      ],
      "metadata": {
        "id": "stage7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}